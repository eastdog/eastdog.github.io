<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Eastdog</title>
    <description>Eastdog&#39;s personal blog</description>
    <link>/</link>
    <atom:link href="/zfeed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Fri, 25 Mar 2016 14:33:57 +0800</pubDate>
    <lastBuildDate>Fri, 25 Mar 2016 14:33:57 +0800</lastBuildDate>
    <generator>Jekyll v3.1.2</generator>
    
      <item>
        <title>从Tensorflow到吐槽Google</title>
        <description>&lt;p&gt;AlphaGo赢了李世石，抛开阴谋论不讲，其实挺让我意外的，本以为怎么着也得第二次才能赢一个世界顶尖棋手吧。不管怎么样当下来看，和语言问题相比棋类问题还是良定义的，所以赢人类只是时间和精力的问题。各种媒体又刷一遍深度学习统治世界，让我觉得很不舒服，人类反正是从来没能从历史里学到任何东西吧，再来一个XX寒冬，我可不想这么快就考虑失业问题。（不过放眼全世界大家经济根本就是在比谁更渣渣所以像我这么高科技的从业者都失业的话经济岂不是一丝一毫增长点都没有了哈哈哈哈）&lt;/p&gt;

&lt;p&gt;扯远了。我对神经网络的了解一直不深，经历了svm大法好–boosting大法好的历程，我对新工具一般都只停留在初级的好奇心（其实是懒）。不过现在DL这么火，工具据说可以很好用，所以就来试试咯。中文关于Tensorflow的tutorial很少（英文的其实也不多），其他开源工具要么是语言不喜欢，要么文档比Tensorflow还短（其实我比较可以理解给MXNet写文档的man hour能创造那么多生产力为毛要浪费这个时间。。。），思前想后（其实也就十分钟）还是决定先试试Tensorflow/skflow。用下来感觉还是不错，不过总是要吐槽的嘛。（心情好的话回头我是不是会写一篇tutorial呢&amp;lt;—-请注意这大概率是一个flag）&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Centos 6 安装基本上搞不定的，至少我搞了一下午也不行。Tensorflow 0.7需要glibc&amp;gt;=2.17，Centos 6最高支持到2.15，2.17试了好久也搞不定。stackoverflow上有两三个个相关问题，有一个方法疑似可行，可是实在是太麻烦太麻烦了。。。github上tensorflow下面有关于这个的issue，因为没啥好的解决方法也已经close掉了。所以如果你在Centos或者其他Linux发行版下安装Tensorflow遇到&lt;code class=&quot;highlighter-rouge&quot;&gt;/lib/libc.so.6: version &#39;GLIBC_2.17&#39; not found&lt;/code&gt;或者类似的问题，强烈建议你直接升级操作系统。&lt;/li&gt;
  &lt;li&gt;skflow是有bug的，github上的几个example没有几个能跑起来的，好歹你们是Google的人喂提交代码的时候用点儿心好不好啊。如果你遇到错误提示是什么&lt;code class=&quot;highlighter-rouge&quot;&gt;list of tensors when single tensor expected&lt;/code&gt;或者哪个list&lt;code class=&quot;highlighter-rouge&quot;&gt;out of range&lt;/code&gt;，检查一下是不是rnn的一个sample方法，丫有一个方法原来返回一个list后来改成list最后一个元素了，可是用到这个方法的地方没改代码 :( 是谁告诉我你们大公司review代码都炒鸡认真的来着。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;其余的想起来再吐吧。&lt;/p&gt;

&lt;p&gt;最后我们再来黑一发Google。最近大公司都在开源自己的工具，这是好事儿，一方面我们有了工具用了，另一方面我们可以看看这些大公司的工具有多难用（其实我觉得原因很简单，因为他们只开源了难用的bug多的工具，人家也不傻）。也许现在Google真的是刷刷题就能进了，写出这种代码的程序员是不是应该吃一壶啊。。。又风闻Google在卖BDI，关注BDI也好几年了（所以我的姿势水平是不是要超出很多科技媒体一大截啊），三年前看踹那个大狗的视频的时候还是很震撼的。很多媒体就喜欢捧Google臭脚（国内尤甚），BMI发视频的时候就在传Google多牛逼多牛逼，把BMI踹走的时候就只报道一句“BMI的机器人技术没法盈利所以Google准备将其售出。”三观不和就不和呗，管理层矛盾就矛盾呗，Google作为一家商业公司这么做天经地义，媒体朋友么，跟我说，Google是一家 商-业-公-司，你们家的情怀多少钱一斤。&lt;/p&gt;

&lt;p&gt;‘Don’t be evil’真的不是随口一说当slogan么，这句话都不能算是良定义的。&lt;/p&gt;
</description>
        <pubDate>Sun, 20 Mar 2016 00:00:00 +0800</pubDate>
        <link>/babble/2016/03/20/tensorflow-babble.html</link>
        <guid isPermaLink="true">/babble/2016/03/20/tensorflow-babble.html</guid>
        
        <category>nn</category>
        
        
        <category>babble</category>
        
      </item>
    
      <item>
        <title>2015下半年读书总结</title>
        <description>&lt;p&gt;下半年工作比较繁忙，专心读书的时间就比较少了。读完的书大部分都是在地铁上用碎片时间看的，基本上都是小说，其他诸如《语言本能》、《GEB》一些书也只是开了个头，又不知何年何月能读完。书总是读不完的，保持读书的乐趣就好。&lt;/p&gt;

&lt;h4 id=&quot;httpbookdoubancomsubject25886175&quot;&gt;&lt;a href=&quot;http://book.douban.com/subject/25886175/&quot;&gt;星际迷航-红衫&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;四星&lt;/p&gt;

&lt;p&gt;对《Star Trek》的印象只停留在小时候的动画片片段（ms会暴露年龄了），好在这本书是独立的故事。情节设置比较新奇，读起来趣味性十足。&lt;/p&gt;

&lt;h4 id=&quot;httpbookdoubancomsubject11527022&quot;&gt;&lt;a href=&quot;http://book.douban.com/subject/11527022/&quot;&gt;华胥引&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;三星&lt;/p&gt;

&lt;p&gt;现在的九州系列都不统一冠名了么？终于发现写不到一起去了么。。。作为爆米花读物还算不错，虽然过几个星期就基本想不起来写的是什么了。。。&lt;/p&gt;

&lt;h4 id=&quot;httpbookdoubancomsubject6847760&quot;&gt;&lt;a href=&quot;http://book.douban.com/subject/6847760/&quot;&gt;随遇而安&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;三星半&lt;/p&gt;

&lt;p&gt;孟非算是挺聪明的一个人，读他的人生经历也算是一种蛮有趣的体验，这本书也还值得一读，就是后面凑数的文章质量再高点儿就好了。&lt;/p&gt;

&lt;h4 id=&quot;httpbookdoubancomsubject25941891httpbookdoubancomsubject25941893&quot;&gt;&lt;a href=&quot;http://book.douban.com/subject/25941891/&quot;&gt;安迪密恩&lt;/a&gt;/&lt;a href=&quot;http://book.douban.com/subject/25941893/&quot;&gt;安迪密恩的觉醒&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;三星半&lt;/p&gt;

&lt;p&gt;我对海伯利安两部的评价还是蛮高的，安迪密恩感觉算是质量上有所下降，也许是前面的故事讲得太好了所以后面有些设定接不上了？海伯利安后面的确有一种神神叨叨的氛围，不过在情境下还算融洽，安迪密恩里面这种神神叨叨的叙述就显得有点儿多且不合理了。&lt;/p&gt;

&lt;h4 id=&quot;httpbookdoubancomsubject1002299&quot;&gt;&lt;a href=&quot;http://book.douban.com/subject/1002299/&quot;&gt;笑傲江湖&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;三星&lt;/p&gt;

&lt;p&gt;金庸早年的作品的确没有后期的成熟啊，还是《天龙八部》写得好。&lt;/p&gt;

&lt;h4 id=&quot;httpbookdoubancomsubject1972412&quot;&gt;&lt;a href=&quot;http://book.douban.com/subject/1972412/&quot;&gt;说谎者的扑克牌&lt;/a&gt;&lt;/h4&gt;

&lt;p&gt;三星半&lt;/p&gt;

&lt;p&gt;花街八卦融入作者个人成长史，放在现在的中国该就是投资界微信大IP。作者大概是可以草根上位了，值得鼓励值得学习哈哈。&lt;/p&gt;

&lt;h4 id=&quot;httpbookdoubancomsubject26650970&quot;&gt;&lt;a href=&quot;http://book.douban.com/subject/26650970/&quot;&gt;古董局中局 4&lt;/a&gt;&lt;/h4&gt;

&lt;p&gt;三星&lt;/p&gt;

&lt;p&gt;祥瑞御免亲王的长篇还能完结按说我们就没啥可说了，不过我觉得我们队亲王的要求的确可以再高一点儿。&lt;/p&gt;

&lt;h4 id=&quot;httpbookdoubancomsubject20397330&quot;&gt;&lt;a href=&quot;http://book.douban.com/subject/20397330/&quot;&gt;小李飞刀1：多情剑客无情剑&lt;/a&gt;&lt;/h4&gt;

&lt;p&gt;两星&lt;/p&gt;

&lt;p&gt;也许古龙真的不是我的菜。我感觉小李飞刀就是个低智商脑残，悲剧都是强行装逼的结果。。。&lt;/p&gt;
</description>
        <pubDate>Mon, 18 Jan 2016 00:00:00 +0800</pubDate>
        <link>/reading/2016/01/18/reading-2015h2.html</link>
        <guid isPermaLink="true">/reading/2016/01/18/reading-2015h2.html</guid>
        
        <category>reading</category>
        
        
        <category>reading</category>
        
      </item>
    
      <item>
        <title>Textrank提取关键词的一些总结</title>
        <description>&lt;p&gt;最近一段时间做的主要工作是从文章中提取关键词，文章基本上是一些公司的介绍，量不算大，大概几万个。这件事儿本身实现起来不算很困难，所以大家也有些各行其是，缺乏一个能达成共识的方法（当然，这个task也比较难以进行评价），不少人也就得过且过，有一个差不多能用的算法就行了。我了解到的方法，基本的如tf-idf，textrank；看到某公司招聘的jd里写了“mi/ig”，我猜是mutual information和information gain，算是比较自然而然的idea；看了几篇文章使用global info来提取关键词（不要求提取的关键词一定在文章中出现，可以是更general的concept），大概是这些思路。&lt;/p&gt;

&lt;p&gt;尝试了一些不同的方法，过于依赖统计的方法我排除了，毕竟我要分析的语料规模可能不是那么大，而且来源于互联网，规范化程度也比较低，纯统计的方法需要太多预处理的工作。最后上线的算法以textrank为基本核心，辅以使用global info的关键词联想算法，后者使用了LSI和比较流行的word2vec。今天主要写一些textrank相关的思考。&lt;/p&gt;

&lt;h1 id=&quot;textrank&quot;&gt;Textrank&lt;/h1&gt;
&lt;p&gt;&lt;a href=&quot;http://web.eecs.umich.edu/~mihalcea/papers/mihalcea.emnlp04.pdf&quot;&gt;Textrank&lt;/a&gt;其实是一个蛮简单的算法，假设我们用textrank来提取关键词，用最简单的话来说，就是给定一篇文章，过滤特定词性的词，指定一个定长的window size，以词为节点，同一个window内的两个词(节点)之间构建一条边(一般建模为双向，边无权)，每篇文章构成了一个图，图中节点数量为文章的词汇表的词汇数。在这个图上运行&lt;a href=&quot;https://en.wikipedia.org/wiki/PageRank&quot;&gt;pagerank&lt;/a&gt;算法(不知道pagerank的点上面的链接，看懂了再回来)，over。&lt;/p&gt;

&lt;p&gt;这篇文章发在04年的emnlp上，思路简单，充满了简洁之美。
作者的原文中，结果最佳的window size是2，收敛系数是0.0001.&lt;/p&gt;

&lt;h1 id=&quot;section&quot;&gt;坑&lt;/h1&gt;
&lt;p&gt;单纯使用Textrank来提取关键词的话，有下面两个问题：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;节点的起始权重都是1，缺少先验信息的支持，文本长度不够长（比如所有的词的词频都是1）的时候，实际没什么产出。&lt;/li&gt;
  &lt;li&gt;文章开始与结尾位置的词有更小的概率被提取，window size越大，文章首尾的单词权重越位置敏感。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;问题1比较好理解，先验信息缺乏导致算法对输入质量的要求比较高，文章的长度最好比较长（词汇丰富，词频差异也会相应增大），短文本上效果不好。&lt;/p&gt;

&lt;p&gt;其实仔细想一下这个算法的idea，在不考虑先验信息和边权重的情况下，文章中tf高的词起始权重比较高，能接到不同词的词有更多的边（同时边没有权重），基本上相当于捏合了基于term frequency和mutual information的关键词提取方法。&lt;/p&gt;

&lt;p&gt;问题2我也是一段时间才发现的，是我自己的想法，还没有完全认证。（stackoverflow上真是很少有人关注算法的问题哈）举例来说，如果window size设为2，那么文章的第一个词&lt;script type=&quot;math/tex&quot;&gt;w_1&lt;/script&gt;和第二个词&lt;script type=&quot;math/tex&quot;&gt;w_2&lt;/script&gt;构建了双向边，&lt;script type=&quot;math/tex&quot;&gt;w_2&lt;/script&gt;和&lt;script type=&quot;math/tex&quot;&gt;w_3&lt;/script&gt;构建了一个双向边，以此类推。比较&lt;script type=&quot;math/tex&quot;&gt;w_2&lt;/script&gt;和&lt;script type=&quot;math/tex&quot;&gt;w_1&lt;/script&gt;可以发现，&lt;script type=&quot;math/tex&quot;&gt;w_2&lt;/script&gt;比&lt;script type=&quot;math/tex&quot;&gt;w_1&lt;/script&gt;多了一次构建边的机会，也就是&lt;script type=&quot;math/tex&quot;&gt;w_2&lt;/script&gt;多了一个incoming的权重，而这个区别仅仅是由于词所在的位置造成的。在文章长度较长的情况下，这个的影响比较小，但是文章长度短的情况下，文章首尾的词有更小的可能性被选为关键词。&lt;/p&gt;

&lt;h1 id=&quot;section-1&quot;&gt;改进尝试&lt;/h1&gt;

&lt;p&gt;我的一个改进尝试就是在初始化图的时候，引入先验权重，先验权重是从全部文章中统计出来的，使用类似tf-idf方式。同时为了让本领域（互联网）关注的词有更高的可能性被提取，也应用领域词表。&lt;/p&gt;

&lt;p&gt;整个先验知识分成两部分，一部分是统计得出的，为词和其对应的权重，权重的值域为[0.2, 1);另一部分是词表得来的，权重设为0.2.也就是在初始化文章对应的图的时候，对每一个词先查看先验知识，更新的权重为原始权重(1)加上先验知识中对应的权重。在Textrank迭代之前，所有节点权重的值域即为[0, 2).&lt;/p&gt;

&lt;h1 id=&quot;section-2&quot;&gt;其他&lt;/h1&gt;

&lt;p&gt;Textrank的实现比较简单，我参考了&lt;a href=&quot;https://github.com/fxsjy/jieba&quot;&gt;jieba&lt;/a&gt;中Textrank的实现，并对其做了一些改进。&lt;/p&gt;
</description>
        <pubDate>Sat, 18 Jul 2015 00:00:00 +0800</pubDate>
        <link>/tech/2015/07/18/keyword-textrank.html</link>
        <guid isPermaLink="true">/tech/2015/07/18/keyword-textrank.html</guid>
        
        <category>ie</category>
        
        <category>keywords</category>
        
        <category>textrank</category>
        
        
        <category>tech</category>
        
      </item>
    
      <item>
        <title>2015上半年读书总结</title>
        <description>&lt;p&gt;年初的时候时间比较充裕，书读得还算比较多，四月的时候换了工作，闲暇时间感觉也懒了一些。最近发现不少东西还只是一知半解，重新翻出来几本比较经典的技术类书籍，希望能再啃下来一次吧。&lt;/p&gt;

&lt;p&gt;上半年小说倒是读了不少，尤其是科幻小说。银河帝国系列的全部，机器人系列的几本，还有几本攒下来碰巧拿起的小说。其他的内容，也就是互联网行业一段时间较火、看起来鸡精含量不算超标的闲书了。&lt;/p&gt;

&lt;h4 id=&quot;httpbookdoubancomsubject10522958&quot;&gt;&lt;a href=&quot;http://book.douban.com/subject/10522958/&quot;&gt;太阳照常升起&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;三星&lt;/p&gt;

&lt;p&gt;前段时间读了《永别了，武器》，算是最近一段时间读过最好的小说了，所以又找了一本海明威的书来读。这本海明威的成名作更符合其“垮掉的一代”的标签，看来海明威还真是一个复杂的人啊。这本书描绘了一些迷惘青年的生活，描写得很透，虽然内容本身不太深刻。不是我最近喜欢的类型吧。&lt;/p&gt;

&lt;h4 id=&quot;httpbookdoubancomsubject23012691&quot;&gt;&lt;a href=&quot;http://book.douban.com/subject/23012691/&quot;&gt;通灵的按摩师&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;四星&lt;/p&gt;

&lt;p&gt;第一次读奈保尔的书，对（那个阶段的）印度人的刻画深入而准确，又不失趣味。多了解一些过去的事儿，看相似的历史，不禁疑问我们到底什么时候才能进步呢？&lt;/p&gt;

&lt;h4 id=&quot;httpbookdoubancomsubject2340609&quot;&gt;&lt;a href=&quot;http://book.douban.com/subject/2340609/&quot;&gt;2001太空漫游&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;三星&lt;/p&gt;

&lt;p&gt;也许是对这个久负盛名的系列期望太高，读完倒觉得比较一般，平稳的叙述里没什么惊喜。在那个时代对未来的想象竟很准确，可仅此而已，缺乏《海伯利安》里那种更深入、宏大、本质的想象力。有机会可以找电影来看看，不知道这种情节比较匮乏的书怎么拍电影。。。&lt;/p&gt;

&lt;h4 id=&quot;httpbookdoubancomsubject6025373&quot;&gt;&lt;a href=&quot;http://book.douban.com/subject/6025373/&quot;&gt;风起陇西&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;三星半&lt;/p&gt;

&lt;p&gt;祥瑞御免。多看在限免，所以就重新读了一遍，好像很少读亲王的书两遍以上。这次感觉没有第一次读那样想要拍案而起，不过亲王对三国历史的了解和想象的确很让人佩服，人物刻画上比三国机密系列稍逊一筹。&lt;/p&gt;

&lt;h4 id=&quot;httpbookdoubancomsubject26297606&quot;&gt;&lt;a href=&quot;http://book.douban.com/subject/26297606/&quot;&gt;从0到1&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;四星&lt;/p&gt;

&lt;p&gt;基本没什么鸡汤，内容虽然不是很新鲜，却也比较贴地气。算是名不虚行了。吐槽一下本书所属的“奇点系列”，彼得·蒂尔本人对库兹韦尔评价不算高，虽然不知道这个系列其他书如何，但是把这本书收到如此系列真是有点儿讽刺。（顺便想下“奇”读什么音，搜狗和百度都认为读qi，我觉得读ji可能更准确，英文对应不是singularity么。。。）
最近又翻了翻自动机的书，回味了一下图灵机可判定性问题等概念，其实就能发现所谓人工智能的奇点，从目前的计算领域来看一点儿影儿都没有。奇点大学的人绝对都不傻，所以里面肯定有人能够指出这一点。那样我只能推断他们坏咯，非傻即奸，问题是，被骗的人是谁呢？&lt;/p&gt;

&lt;h4 id=&quot;httpbookdoubancomsubject1017143&quot;&gt;&lt;a href=&quot;http://book.douban.com/subject/1017143/&quot;&gt;不能承受的生命之轻&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;三星半&lt;/p&gt;

&lt;p&gt;使人生有分量的东西到底是什么？米兰昆德拉小说的奇怪之处在于他总是在叙述过程中跳出来，试图把一个道理娓娓道来。这既让我失去了阅读的快感，也没法细想提出的问题。人和人之间的爱情，或者关系，其起始究竟是不是一种偶然，还是无论如何都会有一个归处？&lt;/p&gt;

&lt;p&gt;从这本书想到这个问题显得很奇怪：自我意识的起源究竟是一个随机的bug，还是有更深层次的原因？如果是前者，人工智能搞不好真的有一天能实现，作为相关从业者倒是有点儿惶恐了。&lt;/p&gt;

&lt;h4 id=&quot;httpbookdoubancomsubject11525217&quot;&gt;&lt;a href=&quot;http://book.douban.com/subject/11525217/&quot;&gt;银河帝国4：基地前奏&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;三星半&lt;/p&gt;

&lt;p&gt;跟一切系列小说一样，第四本就是用来起承转合介绍背景铺垫剧情的。基地4是第一本前传，讲心理史学发明之前谢顿的故事，更像是历险记，不科幻也没什么太多想法。&lt;/p&gt;

&lt;p&gt;本书中描绘的谢顿有血有肉，和前几部中的主人公有些相似的性格和特性。我总觉得这样的人是不可能总结出来心理史学的，心理史学的创造者应该是一个彻头彻尾的理性主义者，本书中的谢顿却很难算作一个，即使是年轻时期，他也并未表现出来理性主义的特质，很难想象他可以转变成基地1中的角色。我不知道阿西莫夫的科学训练如何，我猜想并不足够。当然用现代科学的标准苛责那个年代的人有些过分，基地系列（至少是前面三部）对人类整体运行的社会性思考还是很深刻的。&lt;/p&gt;

&lt;p&gt;不过我更中意海伯利安或者银河系搭车客指南系列。&lt;/p&gt;

&lt;h4 id=&quot;httpbookdoubancomsubject11528306&quot;&gt;&lt;a href=&quot;http://book.douban.com/subject/11528306/&quot;&gt;银河帝国5：迈向基地&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;三星&lt;/p&gt;

&lt;p&gt;这本比第四本还要糟糕一点儿，阿西莫夫的确不太擅长深入刻画人物。谢顿儿子这个角色的意义在哪儿呢，出现得很自然，存在感却有点儿薄弱，突然又死了。。。两本前传把心理史学的发展过程明白得展现了出来，却不让人信服，其实还不如让读者保留着神秘感呐。&lt;/p&gt;

&lt;h4 id=&quot;httpbookdoubancomsubject25928983&quot;&gt;&lt;a href=&quot;http://book.douban.com/subject/25928983/&quot;&gt;周鸿祎自述 : 我的互联网方法论&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;三星&lt;/p&gt;

&lt;p&gt;不管你怎么说周鸿祎，我觉得他有一个很显著的优点就是不(太)装逼。这本书里都是比较明白的道理，虽然不深刻，却不灌鸡汤，周鸿祎也知道自己在说什么，倒给人一种质朴之感。作者知道自己在说什么，这本该是所有书的基本要求，现在却能成为一个弥足珍贵的优点，唏嘘。&lt;/p&gt;

&lt;h4 id=&quot;httpbookdoubancomsubject6709783&quot;&gt;&lt;a href=&quot;http://book.douban.com/subject/6709783/&quot;&gt;浪潮之巅&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;三星半&lt;/p&gt;

&lt;p&gt;吴博士好像一个业余历史研究者，现在比较喜欢总结历史。硅谷公司的兴衰，这本书娓娓道来，倒也不失趣味，吴博士讲故事的能力还是不错的。&lt;/p&gt;

&lt;h4 id=&quot;httpbookdoubancomsubject11528307&quot;&gt;&lt;a href=&quot;http://book.douban.com/subject/11528307/&quot;&gt;银河帝国6：基地边缘&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;三星半&lt;/p&gt;

&lt;p&gt;不少优秀的科幻小说都有一个共同的点：对未来人性的精彩描摹。我想这源自作者对历史和人性的深刻把握，人类科技将无可争议地发展进步下去，人性的变化又会如何，社会的结构会怎么变化？悲观如我，不会用“进步”这个词儿来形容人性演化吧。&lt;/p&gt;

&lt;p&gt;这本书的新主角，拥有最亮主角光环，将决定整个宇宙的发展方向了。&lt;/p&gt;

&lt;h4 id=&quot;httpbookdoubancomsubject1015452&quot;&gt;&lt;a href=&quot;http://book.douban.com/subject/1015452/&quot;&gt;国境以南 太阳以西&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;三星半&lt;/p&gt;

&lt;p&gt;我对村上的书的评价，总在[精彩，精神病]之间跳跃，这本书不得不说有点儿偏后者，也许是日本人的世界观和我有点儿不一样。&lt;/p&gt;

&lt;p&gt;对了，这本书其实我很早以前读过，快读完的时候才发现。。。&lt;/p&gt;

&lt;h4 id=&quot;httpbookdoubancomsubject1860750&quot;&gt;&lt;a href=&quot;http://book.douban.com/subject/1860750/&quot;&gt;了不起的盖茨比&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;三星&lt;/p&gt;

&lt;p&gt;看了一遍书，又看了一遍电影，感觉还是电影比较精彩。
旧钱永远鄙视新钱，只是因为旧钱已经不能维系了吧。&lt;/p&gt;

&lt;h4 id=&quot;httpbookdoubancomsubject11528308&quot;&gt;&lt;a href=&quot;http://book.douban.com/subject/11528308/&quot;&gt;银河帝国7：基地与地球&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;四星&lt;/p&gt;

&lt;p&gt;大团圆结局啦。我早就猜到丹尼尔才是幕后黑手。&lt;/p&gt;

&lt;h4 id=&quot;httpbookdoubancomsubject3127862&quot;&gt;&lt;a href=&quot;http://book.douban.com/subject/3127862/&quot;&gt;王子与贫儿&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;三星&lt;/p&gt;

&lt;p&gt;小时候超级喜欢汤姆索亚和哈克贝利芬系列，也许是我长大了吧，对这种叙事的童话不怎么感兴趣了。&lt;/p&gt;

&lt;h4 id=&quot;httpbookdoubancomsubject25803923&quot;&gt;&lt;a href=&quot;http://book.douban.com/subject/25803923/&quot;&gt;你是我不及的梦&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;两星&lt;/p&gt;

&lt;p&gt;买之前就猜是圈钱的，读起来发现果然是。
本书集结了三毛之前未发表的文章。—-未发表是有原因的，我猜是三毛对它们不太满意。&lt;/p&gt;

&lt;h4 id=&quot;httpbookdoubancomsubject20390695&quot;&gt;&lt;a href=&quot;http://book.douban.com/subject/20390695/&quot;&gt;银河帝国8：我，机器人&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;四星&lt;/p&gt;

&lt;p&gt;开始了机器人系列。阿西莫夫对于人工智能的发展历程，描绘得栩栩如生，让你都要相信这就是未来的确会发生的事儿。有一天我们真的发明了正子脑，再来看阿西莫夫的预言吧。&lt;/p&gt;

&lt;h4 id=&quot;httpbookdoubancomsubject24935042&quot;&gt;&lt;a href=&quot;http://book.douban.com/subject/24935042/&quot;&gt;时间之墟&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;三星&lt;/p&gt;

&lt;p&gt;试读的部分比较精彩，然后我就买了。。。结果虎头蛇尾。&lt;/p&gt;

</description>
        <pubDate>Tue, 30 Jun 2015 00:00:00 +0800</pubDate>
        <link>/reading/2015/06/30/reading-2015h1.html</link>
        <guid isPermaLink="true">/reading/2015/06/30/reading-2015h1.html</guid>
        
        <category>reading</category>
        
        
        <category>reading</category>
        
      </item>
    
      <item>
        <title>使用neo4j进行交易闭环分析</title>
        <description>&lt;p&gt;目前手头有一份一些公司的发票数据，反应了这些公司之间的交易情况。本来主要目的是做一些fraud analysis，看看能不能找出一些虚假/欺诈的交易，进而分析公司的账务情况。&lt;/p&gt;

&lt;p&gt;刚好最近在看neo4j，想试一下graph analysis，数据提供者也觉得分析一下不同公司之间交易闭环也蛮有趣的。试了一下，用neo4j和python的组合来完成这事儿还是非常容易的。&lt;/p&gt;

&lt;h1 id=&quot;section&quot;&gt;问题定义与分析&lt;/h1&gt;
&lt;p&gt;数据是不同商家(我称为dealer)之间的交易(称为transaction)，经过数据清洗和简化，每条数据的格式可以认为是这样：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;dealerA, productA, dealerB, amount
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;表示dealerA将数额为amount的productA卖给了dealerB。数据集中每个dealer即可能是卖家，也可能是买家。要解决的问题是，给定一定量这样的数据，发现交易闭环，即从一个dealer出发，通过不同的交易和其他dealer，最后还有人将产品卖回给最开始的dealer。&lt;/p&gt;

&lt;p&gt;可以把这个问题抽象成一个有向图，dealer是图的节点，transaction是图的边，从卖家指向买家，权重为交易额。要解决的问题即是，给定一个图G(V,E)，找出所有的闭合子图。&lt;/p&gt;

&lt;h1 id=&quot;neo4j&quot;&gt;neo4j实现&lt;/h1&gt;
&lt;p&gt;这个问题可以全程使用neo4j实现，不过方便起见，还是用了python做胶水。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;首先，根据输入数据构造交易图谱，得到一个neo4j的数据库。&lt;/li&gt;
  &lt;li&gt;第二步，使用neo4j的查询语言Cypher进行封闭子图查询。&lt;/li&gt;
  &lt;li&gt;最后判断重复结果，输出。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;构造图的过程中要注意的问题就是不能重复创建节点。Cypher中得merge/create unique语法其实不是特别方便，两者的语义都不是我想要实现的“创建关系r(x&lt;sub&gt;1&lt;/sub&gt;, x&lt;sub&gt;2&lt;/sub&gt;)及节点x&lt;sub&gt;1&lt;/sub&gt;,x&lt;sub&gt;2&lt;/sub&gt;，如果x&lt;sub&gt;1&lt;/sub&gt;/x&lt;sub&gt;2&lt;/sub&gt;存在则不重复创建”，只好分别对x&lt;sub&gt;1&lt;/sub&gt;，x&lt;sub&gt;2&lt;/sub&gt;和r进行merge操作。&lt;/p&gt;

&lt;p&gt;构造的代码如下，商家定义为&lt;code class=&quot;highlighter-rouge&quot;&gt;Dealer&lt;/code&gt;类，交易关系命名为&lt;code class=&quot;highlighter-rouge&quot;&gt;s2&lt;/code&gt;，函数的输入&lt;code class=&quot;highlighter-rouge&quot;&gt;connection&lt;/code&gt;为一个本地的neo4j数据库连接；输入文件格式如前述，第一列和第三列为卖方和买房的id。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;create_graph&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inpath&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;connection&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;codecs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inpath&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;encoding&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;utf-8&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;cursor&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;connection&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cursor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;line&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;entries&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;line&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\t&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;entries&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;strip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;entries&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;strip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;cursor&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;execute&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;MERGE (x:Dealer {id:{0}}) &quot;&lt;/span&gt;
                           &lt;span class=&quot;s&quot;&gt;&quot;MERGE (y:Dealer {id:{1}}) &quot;&lt;/span&gt;
                           &lt;span class=&quot;s&quot;&gt;&quot;MERGE (x) -[:s2]-&amp;gt; (y)&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;connection&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;commit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;最终输出的时候要判断找到的子图是不是重复的，由于起点不同，所以查询结果可能有重复。在这里我定义了一个闭合子图类，定义了它的哈希方法和判断相等的方法。还有一个需要注意的地方是，设定一个节点做起始节点，将找到的两个子图结果拼接起来仍是一个合法的结果，例如找到两个结果&lt;code class=&quot;highlighter-rouge&quot;&gt;a--b--a&lt;/code&gt;和&lt;code class=&quot;highlighter-rouge&quot;&gt;a--c--a&lt;/code&gt;，拼接起来的结果&lt;code class=&quot;highlighter-rouge&quot;&gt;a--b--a--c--a&lt;/code&gt;仍是一个合法的图查询结果，但是不是我们想要的，所以还需要过滤掉这种倒8字型的子图。这个&lt;code class=&quot;highlighter-rouge&quot;&gt;Circle&lt;/code&gt;类的代码如下。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Circle&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;object&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nodes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;

        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;valid&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nodes&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;count&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nodes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;valid&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nodes&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nodes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:]&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__hash__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;

        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;frozenset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nodes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__hash__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nodes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__hash__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__eq__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;other&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;

        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__hash__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;other&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__hash__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;isinstance&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;other&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Circle&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;toString&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;

        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;-&amp;gt;&#39;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;join&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nodes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;查询并将结果写入文件：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;search4circle&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;outpath&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;connection&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;cursor&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;connection&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cursor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;circles&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;codecs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;outpath&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;w&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;utf-8&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fo&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cursor&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;execute&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;START n=node(*) &quot;&lt;/span&gt;
                                &lt;span class=&quot;s&quot;&gt;&quot;MATCH p=n-[:s2*2..10]-&amp;gt;n &quot;&lt;/span&gt;
                                &lt;span class=&quot;s&quot;&gt;&quot;RETURN nodes(p)&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;circles&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;map&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;id&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;fo&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;write&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;join&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;map&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;toString&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt;
                               &lt;span class=&quot;nb&quot;&gt;filter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;valid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                      &lt;span class=&quot;nb&quot;&gt;set&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;map&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Circle&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;circles&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))))))&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;connection&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;commit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;我指定的子图边的数量是2-10个，交易系统更复杂、闭环长度可能更长的时候应该适当增加上界。我的数据只是toy级别的，几万条交易数据而已，跑下来不到一分钟，不算很短了。具体原因我没有深究，我猜测是查询之前建立索引比较耽误工夫，因为不同边长度的结果几乎是一起出来的。如果是建索引比较花时间，那就不太会影响线上使用的速度。&lt;/p&gt;

&lt;p&gt;总体感觉上，Cypher作为查询语言不算难用，相较于SPARQL，更类似SQL。性能上，neo4j是跑在jvm上的，应该优化不算深，也有不少人诟病neo4j的速度，不过可能是我的数据量小，性能瓶颈不是很明显，至少不比Jena之类的框架差。&lt;/p&gt;

&lt;p&gt;一段时间内应该不会做太多知识图谱相关的工作了，所以之前更深入研究graphdb的计划可能就要放慢节奏了，更多地会在个人兴趣项目上对相关内容进行一些尝试。&lt;/p&gt;
</description>
        <pubDate>Fri, 24 Apr 2015 00:00:00 +0800</pubDate>
        <link>/tech/2015/04/24/neo4j-closed-cycle.html</link>
        <guid isPermaLink="true">/tech/2015/04/24/neo4j-closed-cycle.html</guid>
        
        <category>neo4j</category>
        
        <category>graphdb</category>
        
        
        <category>tech</category>
        
      </item>
    
      <item>
        <title>基于CRF的中文分词解码器(Java实现)</title>
        <description>&lt;p&gt;本文主要介绍使用CRF(&lt;a href=&quot;http://en.wikipedia.org/wiki/Conditional_random_field&quot;&gt;Conditional Random Field&lt;/a&gt;，条件随机场)来进行中文分词的工作，着重介绍解码器的实现。&lt;/p&gt;

&lt;h1 id=&quot;crf&quot;&gt;什么是CRF&lt;/h1&gt;

&lt;p&gt;CRF是现在比较流行的序列标注算法，其他的序列标注算法例如HMM(Hidden Makov Model，隐马)/MEMM(Maximum-entropy Markov model，最大熵)，三者主要的差别在于HMM考虑转移概率(transition probability)和生成概率(emission probability)，二者分布的计算是独立的；MEMM和HMM类似，但是其转移概率是基于输出的条件概率；CRF更为复杂，其特征抽取自全部篇幅(具体由特征函数确定)，标注的考量更全局化。具体训练上，HMM比较容易，使用最大似然在语料中进行统计即可。
&lt;br /&gt;上面三种方法都可以进行序列标注，序列标注在NLP里有很多应用，比如中文分词，词性标注，命名实体分析，chunk（分块？不知道这个中文一般是怎么翻译的）等等。CRF一个比较显著的优点在于对于未登录词的识别效果不错，这里可能有一个比较容易混淆的点，未登录词指的是训练语料中没出现的词，并不是所谓的“新词”（比如网络流行语）。假如新词的构词法和训练语料不太相似，我认为识别的效果不会太理想。（新词的识别有一些其他的方法）&lt;/p&gt;

&lt;h1 id=&quot;section&quot;&gt;工具与模型训练&lt;/h1&gt;

&lt;p&gt;本文介绍的方法使用到的工具方法如下，
&lt;br /&gt;语料：北大人民日报语料，6个月，其中前5个月用于训练，第6个月做测试
&lt;br /&gt;工具：crf++用于训练模型，java写了一个解码和分词的程序，python脚本用于把语料组织成crf++的格式。&lt;/p&gt;

&lt;p&gt;crf++网上有不少教程，其本身也比较容易上手，就不多介绍了。
&lt;br /&gt;crf++训练过程中的内存管理其实是有一些问题的，我感觉应该是词语统计过程中有一部分内存没有释放，c++基础也不太好，没去深究代码，在一台内存大点的机器上跑就好了。还可以考虑的一个trick是训练时候使用 &lt;code class=&quot;highlighter-rouge&quot;&gt;-f&lt;/code&gt; 参数，过滤掉频率过小的特征。
&lt;br /&gt;训练结果为一个模型文件，模型包含5个部分：
&lt;br /&gt;1. 文件头，包含模型信息，比如maxid是模型中特征的数量
&lt;br /&gt;2. 标签集合，在分词这个任务里我使用了5个tag，B/E/M&lt;sub&gt;1&lt;/sub&gt;/M&lt;sub&gt;2&lt;/sub&gt;/S
&lt;br /&gt;3. 特征模板，最后一个如果是B表示使用Bigram特征
&lt;br /&gt;4. 特征索引，为一个整数和一个特征。比如 &lt;code class=&quot;highlighter-rouge&quot;&gt;345 U00:严&lt;/code&gt; 的意思是特征 &lt;code class=&quot;highlighter-rouge&quot;&gt;U00:严&lt;/code&gt; 的索引开始于345，即&lt;code class=&quot;highlighter-rouge&quot;&gt;F(B|U00:严)=feature_values[345]&lt;/code&gt;，同理，2中的下一个标签E在这个特征上对应的值的索引是346
&lt;br /&gt;5. 特征对应的值，为一个浮点数。通过上面的索引来查找。&lt;/p&gt;

&lt;p&gt;一个可能的训练模型看起来是这样的,&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;version: 100
&amp;lt;br&amp;gt;cost-factor: 1
&amp;lt;br&amp;gt;maxid: 6512970
xsize: 1

B
E
M1
M2
S

U00:%x[-2,0]
U01:%x[-1,0]
U02:%x[0,0]
U03:%x[1,0]
U04:%x[2,0]
U05:%x[-2,0]/%x[-1,0]/%x[0,0]
U06:%x[-1,0]/%x[0,0]/%x[1,0]
U07:%x[0,0]/%x[1,0]/%x[2,0]
U08:%x[-1,0]/%x[0,0]
U09:%x[0,0]/%x[1,0]
B

0 B
25 U00:_B-1
30 U00:_B-2
35 U00:±
40 U00:·
45 U00:×
50 U00:—
55 U00:‘
60 U00:’
65 U00:“
70 U00:”
75 U00:…
80 U00:‰
85 U00:℃
...

0.0001855083586074
-0.0000550467674851
-0.0000523310422437
-0.0000072055131253
-0.0000045287760644
0.0001191120946419
-0.0000618634324638
-0.0000507848415768
-0.0000074019745383
-0.0000044408072127
0.0001244910548052
-0.0002948948488240
-0.0002653346476889
-0.0000384288559099
-0.0000228130525827
0.0006214713986071
-0.0000890289555236
...
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;根据输入和特征模板怎么生成特征我就不细说了，看一下就明白了。B_1表示句首的前一位。&lt;/p&gt;

&lt;h1 id=&quot;section-1&quot;&gt;解码&lt;/h1&gt;

&lt;p&gt;因为公司的系统是用Java开发的，所以为了分词专门用Java写了一个分词器，主要就是模型的解码。&lt;/p&gt;

&lt;p&gt;解码首先要把模型读入内存中，这其实是一个比较tricky的问题，网上也没有太多的介绍。存储要能实现o(1)的查找速度，这样才能保证分词的效率。对于序列标注问题来说，使用比较多的方法是用DATrie来实现，但是我考虑了一下，我要查找的内容(形如&lt;code class=&quot;highlighter-rouge&quot;&gt;U00:丛&lt;/code&gt;)比较规则，除了前面特征模板的索引以外(U00)，剩下的部分比较短（最长三个字），如果构建Trie树，其形状也是很扁平的。所以影响效率更主要的因素还是哈希函数的效率。最终我没使用DATrie(懒得写了。。。)，而是设计了一种查找结构，首先对特征模板的索引进行区分(只有10个特征模板)，针对每一个特征模板，使用一个哈希表来储存其中的字符和其对应的整数索引。每一个哈希表大概有10w个键值对，对性能的提高可以直接根据键值对的数量来优化哈希函数。&lt;/p&gt;

&lt;p&gt;模型读入以后就比较容易了。对一个输入句子进行分词包含以下步骤，
&lt;br /&gt;1. 句子字符进行预处理。数字的处理，标点的处理，半角全角的转换。
&lt;br /&gt;2. 根据特征模板对每一个字符生成一系列特征。
&lt;br /&gt;3. 初始化一个m*n的矩阵，m为句子中字符的数量，n为tag的数量。
&lt;br /&gt;4. 根据特征函数计算矩阵中每一个node的得分/概率。
&lt;br /&gt;5. 如果有bigram feature，计算输出tag的序列，可以使用viterbi算法，很简单。
&lt;br /&gt;6. 输出tag序列，根据序列提供分词结果。&lt;/p&gt;

&lt;p&gt;然后给公司里的编辑们写一个GUI。。。不得不说，Java写出来的GUI蛮丑的。&lt;/p&gt;

&lt;h1 id=&quot;section-2&quot;&gt;评估&lt;/h1&gt;

&lt;p&gt;分词速度比较一般，查找的部分还有可以优化的地方。
&lt;br /&gt;现在的分词速度大概是40字符/ms左右，一般情况下够用了。
&lt;br /&gt;&lt;em&gt;（不够用的时候，开两个进程，反正内存消耗也不大，还不够？开五个。。。）&lt;/em&gt;
&lt;br /&gt;使用人民日报第6个月的语料进行测试，
&lt;br /&gt;字标注的准确率97.57%；词语的准确率和召回都是97.16%左右。看起来还不错。&lt;/p&gt;

&lt;p&gt;好久没写Java了写起来看着。。。好长。&lt;/p&gt;
</description>
        <pubDate>Sat, 20 Dec 2014 00:00:00 +0800</pubDate>
        <link>/tech/2014/12/20/CRF-word-segmentation.html</link>
        <guid isPermaLink="true">/tech/2014/12/20/CRF-word-segmentation.html</guid>
        
        <category>CRF</category>
        
        <category>分词</category>
        
        <category>Java</category>
        
        
        <category>tech</category>
        
      </item>
    
  </channel>
</rss>
