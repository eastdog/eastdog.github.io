<!DOCTYPE html>
<html>

<head>

<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width initial-scale=1">

<title>Eastdog</title>
<meta name="description" content="Freshman21 is a Jekyll blog theme.">
<meta name="keywords" content="nlp, tech">

<link rel="icon" href="/images/favicon.ico">
<link rel="stylesheet" href="/css/main.css">
<link rel="canonical" href="/">
<link rel="alternate" type="application/atom+xml" title="Eastdog" href="/zfeed.xml" />

<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


</head>


<body>

<div class="container">

  <header class="site-header">

  <div class="wrapper">

    <h1 class="site-title"><a href="/">Eastdog</a></h1>
    <h3 class="site-meta">Eastdog is not dog</h3>

    <nav class="site-nav">
      <a href="#" class="menu-icon">
        <svg viewBox="0 0 18 15">
          <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
      </a>
      <div class="trigger">
        
        
        <a class="page-link" href="/">Home</a>
        
        
        
        <a class="page-link" href="/about/">About</a>
        
        
        
        <a class="page-link" href="/archives/">Archives</a>
        
        
        
        <a class="page-link" href="/categories/">Categories</a>
        
        
        
        <a class="page-link" href="/tags/">Tags</a>
        
        
        
        <a class="page-link" href="/guestbook/">Guestbook</a>
        
        
        
        <a class="page-link" href="/feed.xml">Subscribe</a>
        
        
        
        
        
        
      </div>
    </nav>

  </div>

</header>


  

  <div class="page-content">
    <div class="wrapper">
      <div class="home">
  <div class="post">
    
    
  
    <header class="post-header">
      <h1 class="post-title">
        <a class="post-link" href="/database/2015/04/24/neo4j-closed-cycle.html">使用neo4j进行交易闭环分析</a>
      </h1>
      <p class="post-meta">
      Posted in
      
      <a href="/categories/#database">database</a>&nbsp;
      
      
      and tagged
      
      <a href="/tags/#neo4j" title="neo4j">neo4j </a>, 
      
      <a href="/tags/#graphdb" title="graphdb">graphdb </a>
      
      
      on Apr 24, 2015
      </p>
    </header>

    <article class="post-content">
      <p>目前手头有一份一些公司的发票数据，反应了这些公司之间的交易情况。本来主要目的是做一些fraud analysis，看看能不能找出一些虚假/欺诈的交易，进而分析公司的账务情况。</p>

<p>刚好最近在看neo4j，想试一下graph analysis，数据提供者也觉得分析一下不同公司之间交易闭环也蛮有趣的。试了一下，用neo4j和python的组合来完成这事儿还是非常容易的。</p>

<h1 id="section">问题定义与分析</h1>
<p>数据是不同商家(我称为dealer)之间的交易(称为transaction)，经过数据清洗和简化，每条数据的格式可以认为是这样：</p>

<pre><code>dealerA, productA, dealerB, amount
</code></pre>

<p>表示dealerA将数额为amount的productA卖给了dealerB。数据集中每个dealer即可能是卖家，也可能是买家。要解决的问题是，给定一定量这样的数据，发现交易闭环，即从一个dealer出发，通过不同的交易和其他dealer，最后还有人将产品卖回给最开始的dealer。</p>

<p>可以把这个问题抽象成一个有向图，dealer是图的节点，transaction是图的边，从卖家指向买家，权重为交易额。要解决的问题即是，给定一个图G(V,E)，找出所有的闭合子图。</p>

<h1 id="neo4j">neo4j实现</h1>
<p>这个问题可以全程使用neo4j实现，不过方便起见，还是用了python做胶水。</p>

<ol>
  <li>首先，根据输入数据构造交易图谱，得到一个neo4j的数据库。</li>
  <li>第二步，使用neo4j的查询语言Cypher进行封闭子图查询。</li>
  <li>最后判断重复结果，输出。</li>
</ol>

<p>构造图的过程中要注意的问题就是不能重复创建节点。Cypher中得merge/create unique语法其实不是特别方便，两者的语义都不是我想要实现的“创建关系r(x<sub>1</sub>, x<sub>2</sub>)及节点x<sub>1</sub>,x<sub>2</sub>，如果x<sub>1</sub>/x<sub>2</sub>存在则不重复创建”，只好分别对x<sub>1</sub>，x<sub>2</sub>和r进行merge操作。</p>

<p>构造的代码如下，商家定义为<code>Dealer</code>类，交易关系命名为<code>s2</code>，函数的输入<code>connection</code>为一个本地的neo4j数据库连接；输入文件格式如前述，第一列和第三列为卖方和买房的id。</p>

<pre><code>def create_graph(inpath, connection):

    with codecs.open(inpath, encoding='utf-8') as f:
        cursor = connection.cursor()
        for line in f:
            entries = line.split('\t')
            a, b = entries[0].strip(), entries[2].strip()
            cursor.execute("MERGE (x:Dealer {id:{0}}) "
                           "MERGE (y:Dealer {id:{1}}) "
                           "MERGE (x) -[:s2]-&gt; (y)", a, b)
            connection.commit()
</code></pre>

<p>最终输出的时候要判断找到的子图是不是重复的，由于起点不同，所以查询结果可能有重复。在这里我定义了一个闭合子图类，定义了它的哈希方法和判断相等的方法。还有一个需要注意的地方是，设定一个节点做起始节点，将找到的两个子图结果拼接起来仍是一个合法的结果，例如找到两个结果<code>a--b--a</code>和<code>a--c--a</code>，拼接起来的结果<code>a--b--a--c--a</code>仍是一个合法的图查询结果，但是不是我们想要的，所以还需要过滤掉这种倒8字型的子图。这个<code>Circle</code>类的代码如下。</p>

<pre><code>class Circle(object):

    def __init__(self, nodes):

        self.valid = True
        if nodes.count(nodes[0]) &gt; 2: self.valid = False
        self.nodes = nodes[1:]

    def __hash__(self):

        return (frozenset(self.nodes).__hash__(), len(self.nodes)).__hash__()

    def __eq__(self, other):

        return self.__hash__() == other.__hash__() if isinstance(other, Circle) else False

    def toString(self):

        return '-&gt;'.join(self.nodes)
</code></pre>

<p>查询并将结果写入文件：</p>

<pre><code>def search4circle(outpath, connection):

    cursor = connection.cursor()
    circles = []
    with codecs.open(outpath, 'w', 'utf-8') as fo:
        for p in cursor.execute("START n=node(*) "
                                "MATCH p=n-[:s2*2..10]-&gt;n "
                                "RETURN nodes(p)"):
            circles.append(map(lambda x: x.get('id'), p[0]))
        fo.write('\n'.join(map(lambda x: x.toString(),
                               filter(lambda x: x.valid,
                                      set(map(lambda x: Circle(x), circles))))))
    connection.commit()
</code></pre>

<p>我指定的子图边的数量是2-10个，交易系统更复杂、闭环长度可能更长的时候应该适当增加上界。我的数据只是toy级别的，几万条交易数据而已，跑下来不到一分钟，不算很短了。具体原因我没有深究，我猜测是查询之前建立索引比较耽误工夫，因为不同边长度的结果几乎是一起出来的。如果是建索引比较花时间，那就不太会影响线上使用的速度。</p>

<p>总体感觉上，Cypher作为查询语言不算难用，相较于SPARQL，更类似SQL。性能上，neo4j是跑在jvm上的，应该优化不算深，也有不少人诟病neo4j的速度，不过可能是我的数据量小，性能瓶颈不是很明显，至少不比Jena之类的框架差。</p>

<p>一段时间内应该不会做太多知识图谱相关的工作了，所以之前更深入研究graphdb的计划可能就要放慢节奏了，更多地会在个人兴趣项目上对相关内容进行一些尝试。</p>

    </article> <hr />
    
    
  
    <header class="post-header">
      <h1 class="post-title">
        <a class="post-link" href="/reading/2015/03/27/reading-201503.html">2015.03读书笔记</a>
      </h1>
      <p class="post-meta">
      Posted in
      
      <a href="/categories/#reading">reading</a>&nbsp;
      
      
      and tagged
      
      <a href="/tags/#reading" title="reading">reading </a>
      
      
      on Mar 27, 2015
      </p>
    </header>

    <article class="post-content">
      <p>本月读书完成度不高（抽出的时间也不多），只有三本。</p>

<h1 id="httpbookdoubancomsubject26297606"><a href="http://book.douban.com/subject/26297606/">从0到1</a></h1>
<p>四星</p>

<p>基本没什么鸡汤，内容虽然不是很新鲜，却也比较贴地气。算是名不虚行了。
吐槽一下本书所属的“奇点系列”，彼得·蒂尔本人对库兹韦尔评价不算高，虽然不知道这个系列其他书如何，但是把这本书收到如此系列真是有点儿讽刺。（顺便想下“奇”读什么音，搜狗和百度都认为读qi，我觉得读ji可能更准确，英文对应不是singularity么。。。）</p>

<h1 id="httpbookdoubancomsubject1017143"><a href="http://book.douban.com/subject/1017143/">不能承受的生命之轻</a></h1>
<p>三星半</p>

<p>使人生有分量的东西到底是什么？米兰昆德拉小说的奇怪之处在于他总是在叙述过程中跳出来，试图把一个道理娓娓道来。这既让我失去了阅读的快感，也没法细想提出的问题。人和人之间的爱情，或者关系，其起始究竟是不是一种偶然，还是无论如何都会有一个归处？</p>

<p>从这本书想到这个问题显得很奇怪：自我意识的起源究竟是一个随机的bug，还是有更深层次的原因？如果是前者，人工智能搞不好真的能实现，作为相关从业者倒是有点儿惶恐了。</p>

<h1 id="httpbookdoubancomsubject11525217"><a href="http://book.douban.com/subject/11525217/">银河帝国4：基地前奏</a></h1>
<p>三星</p>

<p>跟一切系列小说一样，第四本就是用来起承转合介绍背景铺垫剧情的。基地4是第一本前传，讲心理史学发明之前谢顿的故事，更像是历险记，不科幻也没什么太多想法。</p>

<p>本书中描绘的谢顿有血有肉，和前几部中的主人公有些相似的性格和特性。我总觉得这样的人是不可能总结出来心理史学的，心理史学的创造者应该是一个彻头彻尾的理性主义者，本书中的谢顿却很难算作一个，即使是年轻时期，他也并未表现出来理性主义的特质，很难想象他可以转变成基地1中的角色。我不知道阿西莫夫的科学训练如何，我猜想并不足够。当然用现代科学的标准苛责那个年代的人有些过分，基地系列（至少是前面三部）对人类整体运行的社会性思考还是很深刻的。</p>

<p>不过我更中意海伯利安或者银河系搭车客指南系列。</p>

    </article> <hr />
    
    
  
    <header class="post-header">
      <h1 class="post-title">
        <a class="post-link" href="/reading/2015/02/28/reading-201502.html">2015.02读书笔记</a>
      </h1>
      <p class="post-meta">
      Posted in
      
      <a href="/categories/#reading">reading</a>&nbsp;
      
      
      and tagged
      
      <a href="/tags/#reading" title="reading">reading </a>
      
      
      on Feb 28, 2015
      </p>
    </header>

    <article class="post-content">
      <p>这个月读书不多，基本都是在交通工具上读完的，四本书是《太阳照常升起》，《通灵的按摩师》，《2001太空漫游》和《风起陇西》。</p>

<h1 id="httpbookdoubancomsubject10522958"><a href="http://book.douban.com/subject/10522958/">太阳照常升起</a></h1>
<p>三星</p>

<p>前段时间读了《永别了，武器》，算是最近一段时间读过最好的小说了，所以又找了一本海明威的书来读。这本海明威的成名作更符合其“垮掉的一代”的标签，看来海明威还真是一个复杂的人啊。这本书描绘了一些迷惘青年的生活，描写得很透，虽然内容本身不太深刻。不是我最近喜欢的类型吧。</p>

<h1 id="httpbookdoubancomsubject23012691"><a href="http://book.douban.com/subject/23012691/">通灵的按摩师</a></h1>
<p>四星</p>

<p>第一次读奈保尔的书，对（那个阶段的）印度人的刻画深入而准确，又不失趣味。</p>

<h1 id="httpbookdoubancomsubject2340609"><a href="http://book.douban.com/subject/2340609/">2001太空漫游</a></h1>
<p>三星</p>

<p>也许是对这个久负盛名的系列期望太高，读完倒觉得比较一般，平稳的叙述里没什么惊喜。在那个时代对未来的想象竟很准确，可仅此而已，缺乏《海伯利安》里那种更深入、宏大、本质的想象力。有机会可以找电影来看看，不知道这种情节比较匮乏的书怎么拍电影。。。</p>

<h1 id="httpbookdoubancomsubject6025373"><a href="http://book.douban.com/subject/6025373/">风起陇西</a></h1>
<p>三星半</p>

<p>祥瑞御免。多看在限免，所以就重新读了一遍，好像很少读亲王的书两遍以上。这次感觉没有第一次读那样想要拍案而起，不过亲王对三国历史的了解和想象的确很让人佩服，人物刻画上比三国机密系列稍逊一筹。</p>

    </article> <hr />
    
    
  
    <header class="post-header">
      <h1 class="post-title">
        <a class="post-link" href="/nlp/2014/12/20/CRF-word-segmentation.html">基于CRF的中文分词解码器(Java实现)</a>
      </h1>
      <p class="post-meta">
      Posted in
      
      <a href="/categories/#nlp">nlp</a>&nbsp;
      
      
      and tagged
      
      <a href="/tags/#CRF" title="CRF">CRF </a>, 
      
      <a href="/tags/#分词" title="分词">分词 </a>, 
      
      <a href="/tags/#Java" title="Java">Java </a>
      
      
      on Dec 20, 2014
      </p>
    </header>

    <article class="post-content">
      <p>本文主要介绍使用CRF(<a href="http://en.wikipedia.org/wiki/Conditional_random_field">Conditional Random Field</a>，条件随机场)来进行中文分词的工作，着重介绍解码器的实现。</p>

<h1 id="crf">什么是CRF</h1>
<p>CRF是现在比较流行的序列标注算法，其他的序列标注算法例如HMM(Hidden Makov Model，隐马)/MEMM(Maximum-entropy Markov model，最大熵)，三者主要的差别在于HMM考虑转移概率(transition probability)和生成概率(emission probability)，二者分布的计算是独立的；MEMM和HMM类似，但是其转移概率是基于输出的条件概率；CRF更为复杂，其特征抽取自全部篇幅(具体由特征函数确定)，标注的考量更全局化。具体训练上，HMM比较容易，使用最大似然在语料中进行统计即可。
<br />上面三种方法都可以进行序列标注，序列标注在NLP里有很多应用，比如中文分词，词性标注，命名实体分析，chunk（分块？不知道这个中文一般是怎么翻译的）等等。CRF一个比较显著的优点在于对于未登录词的识别效果不错，这里可能有一个比较容易混淆的点，未登录词指的是训练语料中没出现的词，并不是所谓的“新词”（比如网络流行语）。假如新词的构词法和训练语料不太相似，我认为识别的效果不会太理想。（新词的识别有一些其他的方法）</p>

<h1 id="section">工具与模型训练</h1>
<p>本文介绍的方法使用到的工具方法如下，
<br />语料：北大人民日报语料，6个月，其中前5个月用于训练，第6个月做测试
<br />工具：crf++用于训练模型，java写了一个解码和分词的程序，python脚本用于把语料组织成crf++的格式。</p>

<p>crf++网上有不少教程，其本身也比较容易上手，就不多介绍了。
<br />crf++训练过程中的内存管理其实是有一些问题的，我感觉应该是词语统计过程中有一部分内存没有释放，c++基础也不太好，没去深究代码，在一台内存大点的机器上跑就好了。还可以考虑的一个trick是训练时候使用 <code>-f</code> 参数，过滤掉频率过小的特征。
<br />训练结果为一个模型文件，模型包含5个部分：
<br />1. 文件头，包含模型信息，比如maxid是模型中特征的数量
<br />2. 标签集合，在分词这个任务里我使用了5个tag，B/E/M<sub>1</sub>/M<sub>2</sub>/S
<br />3. 特征模板，最后一个如果是B表示使用Bigram特征
<br />4. 特征索引，为一个整数和一个特征。比如 <code>345 U00:严</code> 的意思是特征 <code>U00:严</code> 的索引开始于345，即<code>F(B|U00:严)=feature_values[345]</code>，同理，2中的下一个标签E在这个特征上对应的值的索引是346
<br />5. 特征对应的值，为一个浮点数。通过上面的索引来查找。</p>

<p>一个可能的训练模型看起来是这样的,</p>

<pre><code>version: 100
&lt;br&gt;cost-factor: 1
&lt;br&gt;maxid: 6512970
xsize: 1

B
E
M1
M2
S

U00:%x[-2,0]
U01:%x[-1,0]
U02:%x[0,0]
U03:%x[1,0]
U04:%x[2,0]
U05:%x[-2,0]/%x[-1,0]/%x[0,0]
U06:%x[-1,0]/%x[0,0]/%x[1,0]
U07:%x[0,0]/%x[1,0]/%x[2,0]
U08:%x[-1,0]/%x[0,0]
U09:%x[0,0]/%x[1,0]
B

0 B
25 U00:_B-1
30 U00:_B-2
35 U00:±
40 U00:·
45 U00:×
50 U00:—
55 U00:‘
60 U00:’
65 U00:“
70 U00:”
75 U00:…
80 U00:‰
85 U00:℃
...

0.0001855083586074
-0.0000550467674851
-0.0000523310422437
-0.0000072055131253
-0.0000045287760644
0.0001191120946419
-0.0000618634324638
-0.0000507848415768
-0.0000074019745383
-0.0000044408072127
0.0001244910548052
-0.0002948948488240
-0.0002653346476889
-0.0000384288559099
-0.0000228130525827
0.0006214713986071
-0.0000890289555236
...
</code></pre>

<p>根据输入和特征模板怎么生成特征我就不细说了，看一下就明白了。B_1表示句首的前一位。</p>

<h1 id="section-1">解码</h1>
<p>因为公司的系统是用Java开发的，所以为了分词专门用Java写了一个分词器，主要就是模型的解码。</p>

<p>解码首先要把模型读入内存中，这其实是一个比较tricky的问题，网上也没有太多的介绍。存储要能实现o(1)的查找速度，这样才能保证分词的效率。对于序列标注问题来说，使用比较多的方法是用DATrie来实现，但是我考虑了一下，我要查找的内容(形如<code>U00:丛</code>)比较规则，除了前面特征模板的索引以外(U00)，剩下的部分比较短（最长三个字），如果构建Trie树，其形状也是很扁平的。所以影响效率更主要的因素还是哈希函数的效率。最终我没使用DATrie(懒得写了。。。)，而是设计了一种查找结构，首先对特征模板的索引进行区分(只有10个特征模板)，针对每一个特征模板，使用一个哈希表来储存其中的字符和其对应的整数索引。每一个哈希表大概有10w个键值对，对性能的提高可以直接根据键值对的数量来优化哈希函数。</p>

<p>模型读入以后就比较容易了。对一个输入句子进行分词包含以下步骤，
<br />1. 句子字符进行预处理。数字的处理，标点的处理，半角全角的转换。
<br />2. 根据特征模板对每一个字符生成一系列特征。
<br />3. 初始化一个m*n的矩阵，m为句子中字符的数量，n为tag的数量。
<br />4. 根据特征函数计算矩阵中每一个node的得分/概率。
<br />5. 如果有bigram feature，计算输出tag的序列，可以使用viterbi算法，很简单。
<br />6. 输出tag序列，根据序列提供分词结果。</p>

<p>然后给公司里的编辑们写一个GUI。。。不得不说，Java写出来的GUI蛮丑的。</p>

<h1 id="section-2">评估</h1>
<p>分词速度比较一般，查找的部分还有可以优化的地方。
<br />现在的分词速度大概是40字符/ms左右，一般情况下够用了。
<br /><em>（不够用的时候，开两个进程，反正内存消耗也不大，还不够？开五个。。。）</em>
<br />使用人民日报第6个月的语料进行测试，
<br />字标注的准确率97.57%；词语的准确率和召回都是97.16%左右。看起来还不错。</p>

<p>好久没写Java了写起来看着。。。好长。</p>

    </article> <hr />
    
    
  
  </div>
</div>

    </div>
  </div>
  <section class="pager">
  
  
</section>



  

  <footer class="site-footer">

  <p>Copyright &copy; <a href="/">Eastdog</a></p>
  <p>Powered by <a href="https://github.com/jekyll/jekyll">Jekyll</a> 
  on 
  
  <a href="https://github.com/">Github</a>
  
  | Theme <a href="https://github.com/yulijia/freshman21/">Freshman21</a> Design by <a href="http://yulijia.net">Lijia Yu</a>  

</footer>


</div>

</body>

</html>
